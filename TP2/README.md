# Deep Learning Project

This is the README for our Deep Learning TP on the context of a project in Image Mining,
during our Master M2 studies in AI, Univerist√© Paris-Saclay.

## Crew members

[@gpspelle](https://github.com/gpspelle)
[@damounayman](https://github.com/damounayman)

## Database

The well-known CIFAR10 database will be used on our experiments.

## Environment

A google colab file was provided by the professor and our goal is to fill it and make
some analyzes about the results we got and also fine tune our hyper parameters.

## Tasks to be accomplished on this README

This readme explain in fair words the work done and present some results. Also,
one may check the code directly instead of reading this file. However, on the code
only the best set of parameters are available and here a deeper analysis on the
used process is described.

### Detailing Part III

TODO: Explain what neural networks andhyper parameters configurations you tested, the resulting performances and trade-offs found

### Loss interpretation

TODO: Provide an interpretationon the losses plot for your top performing configurations and insights on potential improvements.
What is happening when the training and test losses start diverging?

### Best set of parameters

Our very best results were obtained with the following set of parameters:

|         Parameters  | #1  | #2  |
| :-----------------: | :-: | :-: |
|      Learning rate  | 301 | 283 |
|         Batch size  | 301 | 283 |
|       N. of epochs  | 301 | 283 |
|      Test accuracy  | 301 | 283 |
|      Training size  | 301 | 283 |
| N. of feature maps  | 301 | 283 |
| N. of conv. layers  | 301 | 283 |
|     FC layers size  | 301 | 283 |

Many sets are detailed, since the way we change one hyper-paremeter might change the way we need to change another.
